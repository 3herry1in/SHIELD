<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
	google.load("jquery", "1.3.2");
</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>SHIELD: a Specialized dataset for Hybrid blInd forEnsics of worLd leaDers</title>
</head>

<body>
	<br>

	<center>
		<span style="font-size:30px">SHIELD: a Specialized dataset for Hybrid blInd forEnsics of worLd leaDers
		</span>
	</center>

	<br>

	<!-- <table align=center width=900px>
		<tr>
			<td align=center width=80px>
				<center>
					<span style="font-size:18px"><a href="https://liming-jiang.com/">Liming Jiang</a></span>
				</center>
			</td>

			<td align=center width=80px>
				<center>
					<span style="font-size:18px"><a href="https://liren2515.github.io/page/">Ren Li</a></span>
				</center>
			</td>

			<td align=center width=80px>
				<center>
					<span style="font-size:18px"><a href="http://wywu.github.io">Wayne Wu</a></span>
				</center>
			</td>

			<td align=center width=80px>
				<center>
					<span style="font-size:18px"><a
							href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a></span>
				</center>
			</td>

			<td align=center width=80px>
				<center>
					<span style="font-size:18px"><a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change
							Loy</a></span>
				</center>
			</td>

		</tr>
	</table>

	<br> -->

	<!-- <table align=center width=900px>
		<tr>
			<td align=center width=80px>
				<center>
					<span style="font-size:20px">Nanyang Technological University</span>
				</center>
			</td>

			<td align=center width=80px>
				<center>
					<span style="font-size:20px">SenseTime Research</span>
				</center>
			</td>

		</tr>
	</table>

	<br> -->

	<table align=center width=900px>
		<tr>
			<td align=center width=80px>
				<center>
					<span>anonymous</span>
				</center>
			</td>

		</tr>
	</table>


	<!--
	<div class="authors">
	  <a href="http://wywu.github.io">Wayne Wu</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://ai.stanford.edu/~kaidicao/">Kaidi Cao</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en&oi=sra">Cheng Li</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a><sup>3</sup>&nbsp;&nbsp;
	</div>
	<div class="affiliations">
	  <sup>1</sup><a href="https://www.sensetime.com/?lang=en-us">SenseTime Research<br></a>
	  <sup>2</sup><a href="http://svl.stanford.edu/">Stanford University<br></a>
	  <sup>3</sup><a href="http://scse.ntu.edu.sg/Pages/Home.aspx">Nanyang Technological University<br></a>
	</div> -->


	<br>
	<table align=center width=900px>
		<tr>
			<td width=900px>
				<center>
					<a href="images/main.png"><img src="images/main.png" height="400px"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
	</table>

	<br>
	<p style="text-align:justify">
		We present our on-going effort of constructing a large-scale benchmark for face forgery detection. The first
		version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far,
		with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the
		same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale
		and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are
		generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms
		those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains
		manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive
		study that evaluates five representative detection baselines and make a thorough analysis of different settings.
	</p>
	<br><br>	
	<hr>

	<table align=center width=1100>
		<center>
			<h1>Demo</h1>
		</center>
		<!-- <tr>
  	              <center>
					  <iframe width="800" height="450" src="https://www.youtube.com/embed/b6iKqkJht38" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
				  </center>
              </tr> -->
	</table>
	<br><br>
	<hr>

	<!-- Paper -->
	<!-- <table align=center width=1100>
		<center>
			<h1>Paper</h1>
		</center>
		<tr>
			<td><a href="./support/paper.pdf"><img style="height:180px" src="./support/paper_3d.jpg" /></a></td>
			<td><span style="font-size:14pt">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery
					Detection<br><br>
					<i>Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy</i><br><br>
					IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020
			</td>
		</tr>
	</table> -->

	<!-- <table align=center width=400px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="./support/paper.pdf">[PDF]</a>
					</center>
			</td>

			<td><span style="font-size:14pt">
					<center>
						<a href="./support/bibtex.txt">[BibTex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br> -->

	<!-- <table align=center width=1100>
		<center>
			<h1>Challenge Report</h1>
		</center>
		<tr>
			<td><a href="../DFC20/resources/DFC20_Summary.pdf"><img style="height:180px"
						src="../DFC20/resources/report_3d.jpg" /></a></td>
			<td><span style="font-size:14pt">DeeperForensics Challenge 2020 on Real-World Face Forgery Detection:
					Methods and Results<br><br>
					<i>Liming Jiang, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen Change Loy, et
						al.</i><br><br>
					arXiv preprint, 2021
			</td>
		</tr>
	</table> -->

	<!-- <table align=center width=400px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="../DFC20/resources/DFC20_Summary.pdf">[PDF]</a>
					</center>
			</td>

			<td><span style="font-size:14pt">
					<center>
						<a href="../DFC20/resources/bibtex.txt">[BibTex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br> -->

	<!-- <table align=center width=1100>
		<center>
			<h1>Book Chapter</h1>
		</center>
		<tr>
			<td><a href="./support/chapter.pdf"><img height="200px" width="146px"
						src="./support/deepfakes_book.jpg" /></a></td>
			<td><span style="font-size:14pt">DeepFakes Detection: the DeeperForensics Dataset and Challenge<br><br>
					<i>Liming Jiang, Wayne Wu, Chen Qian, Chen Change Loy</i><br><br>
					Digital Face Manipulation and Detection - From DeepFakes to Morphing Attacks, Springer, 2022
			</td>
		</tr>
	</table>

	<table align=center width=400px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="./support/chapter.pdf">[PDF]</a>
					</center>
			</td>

			<td><span style="font-size:14pt">
					<center>
						<a href="./support/chapter_bibtex.txt">[BibTex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br> -->

	<!-- <hr> -->

	<!-- Download -->
	<table align=center width=1100>
		<center>
			<h1>Downloads</h1>
		</center>
		<tr>

			<td>
				<center>
					<!-- <a href= "#"
						class="imageLink"><img src="./support/dataset.png" width="160"></a><br> -->
					<a href="https://github.com/EndlessSora/DeeperForensics-1.0/tree/master/dataset">SHIELD
						Dataset</a>
				</center>
			</td>

			<!-- <td>
				<center>
					<a href="https://github.com/EndlessSora/DeeperForensics-1.0" class="imageLink"><img
							src="./support/github.png" width="160"></a><br>
					<a href="https://github.com/EndlessSora/DeeperForensics-1.0">Code</a>
				</center>
			</td> -->

		</tr>
	</table>
	<br>

	<hr>

	<!--comparison / method / visualization (origin, many-to-many, perturbations)-->
	<table align=center width=1100>
		<center>
			<h1>Comparison</h1>
		</center>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/comparison.png"><img src="./support/comparison.png" width="1100"></img></href>
					</a><br>
				</center>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=1100>
		<center>
			<h1>Manipulation Method</h1>
		</center>
		<tr>
			<td width=1100>
				<center>
					<a href="images/manipulation.png"><img src="images/manipulation.png" width="800"></img></href></a><br>
				</center>
			</td>
		</tr>
	</table>
	<p class="text-justify">
		We also propose a new learning-based <b>many-to-many</b> face swapping method, <b>DeepFake Variational
			Auto-Encoder (DF-VAE)</b>. DF-VAE improves <i>scalability</i>, <i>style matching</i>, and <i>temporal
			continuity</i> to ensure face swapping <b>quality</b>. In training, we reconstruct the source and target
		faces in blue and orange arrows, respectively, by extracting landmarks and constructing an unpaired sample as
		the condition. Optical flow differences are minimized after reconstruction to improve temporal continuity. In
		inference, we swap the latent codes and get the reenacted face in green arrows. Subsequent MAdaIN module fuses
		the reenacted face and the original background resulting in the swapped face.
	</p>
	<br>

	<!-- <hr>
	<table align=center width=1100>
		<center>
			<h1>Visualizations</h1>
		</center>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/data_collection.png"><img src="./support/data_collection.png" width="1100"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/data_collection_more.png"><img src="./support/data_collection_more.png"
							width="1100"></img></href></a><br>
				</center>
			</td>
		</tr>
	</table>
	<center>
		<p class="text-justify">
			Extensive Data Collection
		</p>
	</center>
	<br>

	<table align=center width=1100>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/manipulation.gif"><img src="./support/manipulation.gif" width="1100"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
	</table>
	<center>
		<p class="text-justify">
			Several Face Manipulation Results
		</p>
	</center>
	<br>

	<table align=center width=1100>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/m2m.png"><img src="./support/m2m.png" width="1100"></img></href></a><br>
				</center>
			</td>
		</tr>
	</table>
	<center>
		<p class="text-justify">
			Many-to-Many (Three-to-Three) Face Swapping by a <b>Single</b> Model
		</p>
	</center>
	<br>

	<table align=center width=1100>
		<tr>
			<td width=1100>
				<center>
					<a href="./support/perturbations.png"><img src="./support/perturbations.png" width="1100"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
	</table>
	<center>
		<p class="text-justify">
			Diverse Perturbations in Real World
		</p>
	</center>
	<br> -->

	<hr>

	<table align=center width=1100px>
		<tr>
			<td>
				<center>
					<h1>Acknowledgments</h1>
				</center>
				This work is supported by the SenseTime-NTU Collaboration Project, Singapore MOE AcRF Tier 1
				(2018-T1-002-056), NTU SUG, and NTU NAP. We gratefully acknowledge the exceptional help from Hao Zhu and
				Keqiang Sun for their contribution on source data collection and coordination.
			</td>
		</tr>
	</table>
	<br><br>
</body>

</html>